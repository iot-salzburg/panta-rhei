{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook will help to get started with the Elasticsearch connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run: 2019-07-29 12:24:39.510683 UTC\n"
     ]
    }
   ],
   "source": [
    "print('Last run:', datetime.datetime.utcnow(), 'UTC')  # timezone can't be detected from browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'puSAOdA',\n",
       " 'cluster_name': 'docker-cluster',\n",
       " 'cluster_uuid': 'QOWGKg_3SQeeRys1ICaEvg',\n",
       " 'version': {'number': '6.2.2',\n",
       "  'build_hash': '10b1edd',\n",
       "  'build_date': '2018-02-16T19:01:30.685723Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '7.2.1',\n",
       "  'minimum_wire_compatibility_version': '5.6.0',\n",
       "  'minimum_index_compatibility_version': '5.0.0'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure ES is up and running\n",
    "res = requests.get('http://elasticsearch:9200')\n",
    "r=json.loads(res.content)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jovyan\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "whoami\n",
    "# pip install elasticsearch\n",
    "# already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Elasticsearch([{'host': 'elasticsearch', 'port': 9200}])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to our cluster\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host': 'elasticsearch', 'port': 9200}])\n",
    "es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 6, 'successful': 6, 'skipped': 0, 'failed': 0}\n"
     ]
    }
   ],
   "source": [
    "# Empty search to ensure it is working\n",
    "res = es.search()\n",
    "print(res[\"_shards\"])\n",
    "# res[\"hits\"][\"hits\"][-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display our indices and document types saved in elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      timestamp cluster        status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\n",
      "1564403080 12:24:40  docker-cluster yellow          1         1      6   6    0    0        5             0                  -                 54.5%\n"
     ]
    }
   ],
   "source": [
    "# the prefix \"!\" is the similar to \"%%sh\"-magic, but only valid in one line\n",
    "!curl -XGET 'http://elasticsearch:9200/_cat/health?v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health status index               uuid                   pri rep docs.count docs.deleted store.size pri.store.size\n",
      "green  open   .kibana             RQxwCJ2UQkKjjBgKdTIL9g   1   0          2            1     11.5kb         11.5kb\n",
      "yellow open   eu.dtz.data-2019.07 P6ue_VBAQiSH-LegMlydaA   5   1       4167            0    944.8kb        944.8kb\n"
     ]
    }
   ],
   "source": [
    "# every index is green if it's healthy\n",
    "!curl -XGET 'http://elasticsearch:9200/_cat/indices?v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"error\" : {\n",
      "    \"root_cause\" : [\n",
      "      {\n",
      "        \"type\" : \"index_not_found_exception\",\n",
      "        \"reason\" : \"no such index\",\n",
      "        \"resource.type\" : \"index_or_alias\",\n",
      "        \"resource.id\" : \"logstash-2018.03.01\",\n",
      "        \"index_uuid\" : \"_na_\",\n",
      "        \"index\" : \"logstash-2018.03.01\"\n",
      "      }\n",
      "    ],\n",
      "    \"type\" : \"index_not_found_exception\",\n",
      "    \"reason\" : \"no such index\",\n",
      "    \"resource.type\" : \"index_or_alias\",\n",
      "    \"resource.id\" : \"logstash-2018.03.01\",\n",
      "    \"index_uuid\" : \"_na_\",\n",
      "    \"index\" : \"logstash-2018.03.01\"\n",
      "  },\n",
      "  \"status\" : 404\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -XGET 'http://elasticsearch:9200/logstash-2018.03.01/_mapping?pretty=true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['.kibana', 'doc'], ['eu.dtz.data-2019.07', 'doc']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "output = subprocess.check_output('curl -s -XGET \"http://elasticsearch:9200/_all/_mapping\"', shell=True)\n",
    "output = json.loads(output)\n",
    "\n",
    "indices = list(output.keys())\n",
    "indices = [[index, typ] for index in indices for typ in output.get(index).get(\"mappings\")]\n",
    "list(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problems with finding the document type for a given index pattern\n",
    "#res = es.get(index=\"logstash-2018.02.07\", id=1)\n",
    "#print(res['_source'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method search in module elasticsearch.client:\n",
      "\n",
      "search(index=None, doc_type=None, body=None, params=None) method of elasticsearch.client.Elasticsearch instance\n",
      "    Execute a search query and get back search hits that match the query.\n",
      "    `<http://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html>`_\n",
      "    \n",
      "    :arg index: A comma-separated list of index names to search; use `_all`\n",
      "        or empty string to perform the operation on all indices\n",
      "    :arg doc_type: A comma-separated list of document types to search; leave\n",
      "        empty to perform the operation on all types\n",
      "    :arg body: The search definition using the Query DSL\n",
      "    :arg _source: True or false to return the _source field or not, or a\n",
      "        list of fields to return\n",
      "    :arg _source_exclude: A list of fields to exclude from the returned\n",
      "        _source field\n",
      "    :arg _source_include: A list of fields to extract and return from the\n",
      "        _source field\n",
      "    :arg allow_no_indices: Whether to ignore if a wildcard indices\n",
      "        expression resolves into no concrete indices. (This includes `_all`\n",
      "        string or when no indices have been specified)\n",
      "    :arg allow_partial_search_results: Set to false to return an overall\n",
      "        failure if the request would produce partial results. Defaults to\n",
      "        True, which will allow partial results in the case of timeouts or\n",
      "        partial failures\n",
      "    :arg analyze_wildcard: Specify whether wildcard and prefix queries\n",
      "        should be analyzed (default: false)\n",
      "    :arg analyzer: The analyzer to use for the query string\n",
      "    :arg batched_reduce_size: The number of shard results that should be\n",
      "        reduced at once on the coordinating node. This value should be used\n",
      "        as a protection mechanism to reduce the memory overhead per search\n",
      "        request if the potential number of shards in the request can be\n",
      "        large., default 512\n",
      "    :arg default_operator: The default operator for query string query (AND\n",
      "        or OR), default 'OR', valid choices are: 'AND', 'OR'\n",
      "    :arg df: The field to use as default where no field prefix is given in\n",
      "        the query string\n",
      "    :arg docvalue_fields: A comma-separated list of fields to return as the\n",
      "        docvalue representation of a field for each hit\n",
      "    :arg expand_wildcards: Whether to expand wildcard expression to concrete\n",
      "        indices that are open, closed or both., default 'open', valid\n",
      "        choices are: 'open', 'closed', 'none', 'all'\n",
      "    :arg explain: Specify whether to return detailed information about score\n",
      "        computation as part of a hit\n",
      "    :arg from\\_: Starting offset (default: 0)\n",
      "    :arg ignore_unavailable: Whether specified concrete indices should be\n",
      "        ignored when unavailable (missing or closed)\n",
      "    :arg lenient: Specify whether format-based query failures (such as\n",
      "        providing text to a numeric field) should be ignored\n",
      "    :arg max_concurrent_shard_requests: The number of concurrent shard\n",
      "        requests this search executes concurrently. This value should be\n",
      "        used to limit the impact of the search on the cluster in order to\n",
      "        limit the number of concurrent shard requests, default 'The default\n",
      "        grows with the number of nodes in the cluster but is at most 256.'\n",
      "    :arg pre_filter_shard_size: A threshold that enforces a pre-filter\n",
      "        roundtrip to prefilter search shards based on query rewriting if\n",
      "        the number of shards the search request expands to exceeds the\n",
      "        threshold. This filter roundtrip can limit the number of shards\n",
      "        significantly if for instance a shard can not match any documents\n",
      "        based on it's rewrite method ie. if date filters are mandatory to\n",
      "        match but the shard bounds and the query are disjoint., default 128\n",
      "    :arg preference: Specify the node or shard the operation should be\n",
      "        performed on (default: random)\n",
      "    :arg q: Query in the Lucene query string syntax\n",
      "    :arg request_cache: Specify if request cache should be used for this\n",
      "        request or not, defaults to index level setting\n",
      "    :arg routing: A comma-separated list of specific routing values\n",
      "    :arg scroll: Specify how long a consistent view of the index should be\n",
      "        maintained for scrolled search\n",
      "    :arg search_type: Search operation type, valid choices are:\n",
      "        'query_then_fetch', 'dfs_query_then_fetch'\n",
      "    :arg size: Number of hits to return (default: 10)\n",
      "    :arg sort: A comma-separated list of <field>:<direction> pairs\n",
      "    :arg stats: Specific 'tag' of the request for logging and statistical\n",
      "        purposes\n",
      "    :arg stored_fields: A comma-separated list of stored fields to return as\n",
      "        part of a hit\n",
      "    :arg suggest_field: Specify which field to use for suggestions\n",
      "    :arg suggest_mode: Specify suggest mode, default 'missing', valid\n",
      "        choices are: 'missing', 'popular', 'always'\n",
      "    :arg suggest_size: How many suggestions to return in response\n",
      "    :arg suggest_text: The source text for which the suggestions should be\n",
      "        returned\n",
      "    :arg terminate_after: The maximum number of documents to collect for\n",
      "        each shard, upon reaching which the query execution will terminate\n",
      "        early.\n",
      "    :arg timeout: Explicit operation timeout\n",
      "    :arg track_scores: Whether to calculate and return scores even if they\n",
      "        are not used for sorting\n",
      "    :arg track_total_hits: Indicate if the number of documents that match\n",
      "        the query should be tracked\n",
      "    :arg typed_keys: Specify whether aggregation and suggester names should\n",
      "        be prefixed by their respective types in the response\n",
      "    :arg version: Specify whether to return document version as part of a\n",
      "        hit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(es.search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 0 Hits:\n"
     ]
    }
   ],
   "source": [
    "# The results of metric testdata from the last 1 hour\n",
    "body = {\n",
    "  \"size\": 10,\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": [\n",
    "        {\"range\" : {\n",
    "            \"phenomenonTime\" : {\n",
    "                #\"gte\": \"2018-02-20T09:08:34.230693+00:00\", \n",
    "                \"gte\": \"now-7d\",\n",
    "                \"lte\": \"now\", \n",
    "                \"time_zone\": \"+01:00\"\n",
    "            }\n",
    "        }},\n",
    "        {\"match\": {\n",
    "            \"Datastream.name.keyword\": {\n",
    "                \"query\": \"Airquality DS\"\n",
    "              }\n",
    "        }}\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "res = es.search(index=\"logs*\", body=body)\n",
    "print(\"Got %d Hits:\" % res['hits']['total'])\n",
    "for hit in res['hits']['hits']:\n",
    "    print(\"Timestamp: {}, \\tmetric: {}, \\tresult: {}\\n\".format(hit[\"_source\"][\"phenomenonTime\"], \n",
    "            hit[\"_source\"][\"Datastream\"][\"name\"], hit[\"_source\"][\"result\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 634 Hits:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_index': 'eu.dtz.data-2019.07',\n",
       " '_type': 'doc',\n",
       " '_id': 'vVmwPWwBRoiMMqnEmgEN',\n",
       " '_score': 2.8039846,\n",
       " '_source': {'@version': '1',\n",
       "  'host': 'iot86',\n",
       "  'port': 57760,\n",
       "  'path': 'datastack-adapter/adapter/datastore_adapter.py',\n",
       "  'dayOfWeek': '1',\n",
       "  'Datastream': {'@iot.id': 44,\n",
       "   '@iot.selfLink': 'http://192.168.48.71:8082/v1.0/Datastreams(44)',\n",
       "   'name': 'Current of the Panda Robot'},\n",
       "  'stack_info': None,\n",
       "  'tags': [],\n",
       "  'resultTime': '2019-07-29T12:24:39.587527+00:00',\n",
       "  'type': 'logstash',\n",
       "  'message': '',\n",
       "  'level': 'INFO',\n",
       "  'hourOfDay': '12',\n",
       "  'logger_name': 'datastore-adapter',\n",
       "  '@timestamp': '2019-07-29T12:24:39.589Z',\n",
       "  'phenomenonTime': '2019-07-29T12:24:39.587501+00:00',\n",
       "  'result': 0.16171596689807538}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The results from the last 30 days, but the page size is limited to the size argument.\n",
    "body = {\n",
    "  \"size\": 10000,\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": [\n",
    "        {\"range\" : {\n",
    "            \"phenomenonTime\" : {\n",
    "                #\"gte\": \"2018-02-20T09:08:34.230693+00:00\", \n",
    "                \"gte\": \"now-30d\",\n",
    "                \"lte\": \"now\", \n",
    "                \"time_zone\": \"+01:00\"\n",
    "            }\n",
    "        }},\n",
    "        {\"match\": {\n",
    "            \"Datastream.name.keyword\": {\n",
    "                \"query\": \"Current of the Panda Robot\"\n",
    "              }\n",
    "        }}\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "res = es.search(index=\"eu.dtz.data-*\", body=body)\n",
    "print(\"Got %d Hits:\" % res['hits']['total'])\n",
    "res[\"hits\"][\"hits\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the hits: 5.367188 kB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "634"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Size of the hits: {:2f} kB\".format(sys.getsizeof(res[\"hits\"][\"hits\"])/1024))\n",
    "len(res[\"hits\"][\"hits\"]) # this request is limited to 10000 hits, as the the size may lead to inconviniences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrolling...\n",
      "appending array of size 0\n",
      "Length of the resulting array: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the scroll\n",
    "page = es.search(\n",
    "index = 'eu.dtz.data-*',\n",
    "scroll = '2m',\n",
    "size = 1000,\n",
    "body = {\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": [\n",
    "        {\"range\" : {\n",
    "            \"phenomenonTime\" : {\n",
    "                #\"gte\": \"2018-02-20T09:08:34.230693+00:00\", \n",
    "                \"gte\": \"now-30d\",\n",
    "                \"lte\": \"now\", \n",
    "                \"time_zone\": \"+01:00\"\n",
    "            }\n",
    "        }},\n",
    "        {\"match\": {\n",
    "            \"Datastream.name.keyword\": {\n",
    "                \"query\": \"Current of the Panda Robot\"\n",
    "              }\n",
    "        }}\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "})\n",
    "\n",
    "sid = page['_scroll_id']\n",
    "scroll_size = page['hits']['total']\n",
    "\n",
    "# Start scrolling and append data\n",
    "data = list()\n",
    "while (scroll_size > 0):\n",
    "    print(\"Scrolling...\")\n",
    "    page = es.scroll(scroll_id = sid, scroll = '2m')\n",
    "    # Update the scroll ID\n",
    "    sid = page['_scroll_id']\n",
    "    # Get the number of results that we returned in the last scroll\n",
    "    scroll_size = len(page['hits']['hits'])\n",
    "    print(\"appending array of size \" + str(scroll_size))\n",
    "    dataframe = [[row[\"_source\"][\"phenomenonTime\"], row[\"_source\"][\"Datastream\"][\"name\"], row[\"_source\"][\"result\"]] for row in page['hits']['hits']]\n",
    "    data += dataframe\n",
    "    # Do something with the obtained page\n",
    "print(\"Length of the resulting array:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-80a4aab09c73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=[\"phenomenonTime\", \"name\", \"result\"])\n",
    "df.index = pd.to_datetime(df[\"phenomenonTime\"])\n",
    "df = df.drop(\"phenomenonTime\", axis=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"currentsPanda.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min: {}, max: {}, mu: {}, std: {}\".format(\n",
    "df[\"result\"].min(), df[\"result\"].max(), df[\"result\"].mean(), df[\"result\"].std()))\n",
    "df[\"result\"].plot.hist(bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline for the dataframe\n",
    "df[\"result\"].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new Index from the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = {\n",
    "    'author': 'kimchy',\n",
    "    'text': 'Elasticsearch: cool. bonsai cool.',\n",
    "    'timestamp': datetime.datetime.now(),\n",
    "}\n",
    "res = es.index(index=\"test-index\", doc_type='tweet', id=1, body=doc)\n",
    "#print(res['created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the version iterates over the data\n",
    "es.get(index=\"test-index\", doc_type='tweet', id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.refresh(index=\"test-index*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connector to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create pyspark-dataFrame\n",
    "sqlCtx = pyspark.SQLContext(sc)\n",
    "sdf = sqlCtx.createDataFrame(df.astype(str))\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.rdd.first()  # [\"host\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView(\"tempTable\")\n",
    "res = sqlCtx.sql(\"\"\"SELECT name, stddev(result) as std__of_result\n",
    "            FROM tempTable\n",
    "            GROUP BY name\"\"\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in Spark directly from Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(elasticsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlCtx.read.format(\"es\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PYSPARK_PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sniffing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# you can specify to sniff on startup to inspect the cluster and load\n",
    "# balance across all nodes\n",
    "# you can also sniff periodically and/or after failure:\n",
    "es = Elasticsearch([{'host': 'elasticsearch', 'port': 9200}],\n",
    "          sniff_on_start=True,\n",
    "          sniff_on_connection_fail=True,\n",
    "          sniffer_timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
